{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions To Run and Background \n",
    "\n",
    "This notebook represents the work done to execute **Deep Q-Learning** on the Flappy Bird enviornment. Due to the niche nature of the Flappy Bird game, assets/ folder is included as this loads the game assets, and the notebook will not be able to run without it.\n",
    "\n",
    "The point of this notebook is demonstrate the training process as well as how to get started with testing an existing model. To begin, install the following: \n",
    "1. Have a Python 3.8.5 Environment Installed, as well as Tensorflow, Pygame. \n",
    "\n",
    "\n",
    "If you ever encounter the following error: `NotFoundError: Key Variable_10 not found in checkpoint` you must restart the notebook kernel as this is an error relating to a non-terminating tensorflow session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we need to set up the PyGame and Flappy Bird enviornment. \n",
    "The source code for this is attributed to: https://github.com/sourabhv/FlapPyBird\n",
    "\n",
    "The original game of Flappy Bird is actually taken down from the market, as the original creator felt it was too addicting. So, a clone must be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.8.0)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "\n",
    "# disable rendering \n",
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "def load():\n",
    "    # path of player with different states\n",
    "    PLAYER_PATH = (\n",
    "            'assets/sprites/redbird-upflap.png',\n",
    "            'assets/sprites/redbird-midflap.png',\n",
    "            'assets/sprites/redbird-downflap.png'\n",
    "    )\n",
    "\n",
    "    # path of background\n",
    "    BACKGROUND_PATH = 'assets/sprites/background-black.png'\n",
    "\n",
    "    # path of pipe\n",
    "    PIPE_PATH = 'assets/sprites/pipe-green.png'\n",
    "\n",
    "    IMAGES, SOUNDS, HITMASKS = {}, {}, {}\n",
    "\n",
    "    # numbers sprites for score display\n",
    "    IMAGES['numbers'] = (\n",
    "        pygame.image.load('assets/sprites/0.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/1.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/2.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/3.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/4.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/5.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/6.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/7.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/8.png').convert_alpha(),\n",
    "        pygame.image.load('assets/sprites/9.png').convert_alpha()\n",
    "    )\n",
    "\n",
    "    # base (ground) sprite\n",
    "    IMAGES['base'] = pygame.image.load('assets/sprites/base.png').convert_alpha()\n",
    "\n",
    "    # sounds\n",
    "    if 'win' in sys.platform:\n",
    "        soundExt = '.wav'\n",
    "    else:\n",
    "        soundExt = '.ogg'\n",
    "        \n",
    "    # select random background sprites\n",
    "    IMAGES['background'] = pygame.image.load(BACKGROUND_PATH).convert()\n",
    "\n",
    "    # select random player sprites\n",
    "    IMAGES['player'] = (\n",
    "        pygame.image.load(PLAYER_PATH[0]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[1]).convert_alpha(),\n",
    "        pygame.image.load(PLAYER_PATH[2]).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # select random pipe sprites\n",
    "    IMAGES['pipe'] = (\n",
    "        pygame.transform.rotate(\n",
    "            pygame.image.load(PIPE_PATH).convert_alpha(), 180),\n",
    "        pygame.image.load(PIPE_PATH).convert_alpha(),\n",
    "    )\n",
    "\n",
    "    # hismask for pipes\n",
    "    HITMASKS['pipe'] = (\n",
    "        getHitmask(IMAGES['pipe'][0]),\n",
    "        getHitmask(IMAGES['pipe'][1]),\n",
    "    )\n",
    "\n",
    "    # hitmask for player\n",
    "    HITMASKS['player'] = (\n",
    "        getHitmask(IMAGES['player'][0]),\n",
    "        getHitmask(IMAGES['player'][1]),\n",
    "        getHitmask(IMAGES['player'][2]),\n",
    "    )\n",
    "\n",
    "    return IMAGES, SOUNDS, HITMASKS\n",
    "\n",
    "def getHitmask(image):\n",
    "    \"\"\"returns a hitmask using an image's alpha.\"\"\"\n",
    "    mask = []\n",
    "    for x in range(image.get_width()):\n",
    "        mask.append([])\n",
    "        for y in range(image.get_height()):\n",
    "            mask[x].append(bool(image.get_at((x,y))[3]))\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    }
   ],
   "source": [
    "# This code will allow for the Game State to be declared. \n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import random\n",
    "import pygame\n",
    "import pygame.surfarray as surfarray\n",
    "from pygame.locals import *\n",
    "from itertools import cycle\n",
    "\n",
    "# disable rendering \n",
    "import os\n",
    "os.environ['SDL_VIDEODRIVER']='dummy'\n",
    "\n",
    "pygame.init()\n",
    "\n",
    "# FPS set to 60 for fast computation. \n",
    "FPS = 60\n",
    "SCREENWIDTH  = 288\n",
    "SCREENHEIGHT = 512\n",
    "\n",
    "FPSCLOCK = pygame.time.Clock()\n",
    "SCREEN = pygame.display.set_mode((SCREENWIDTH, SCREENHEIGHT))\n",
    "pygame.display.set_caption('Flappy Bird')\n",
    "\n",
    "IMAGES, SOUNDS, HITMASKS = load()\n",
    "PIPEGAPSIZE = 100 # gap between upper and lower part of pipe\n",
    "BASEY = SCREENHEIGHT * 0.79\n",
    "\n",
    "PLAYER_WIDTH = IMAGES['player'][0].get_width()\n",
    "PLAYER_HEIGHT = IMAGES['player'][0].get_height()\n",
    "PIPE_WIDTH = IMAGES['pipe'][0].get_width()\n",
    "PIPE_HEIGHT = IMAGES['pipe'][0].get_height()\n",
    "BACKGROUND_WIDTH = IMAGES['background'].get_width()\n",
    "\n",
    "PLAYER_INDEX_GEN = cycle([0, 1, 2, 1])\n",
    "\n",
    "\n",
    "class GameState:\n",
    "    def __init__(self):\n",
    "        self.score = self.playerIndex = self.loopIter = 0\n",
    "        self.playerx = int(SCREENWIDTH * 0.2)\n",
    "        self.playery = int((SCREENHEIGHT - PLAYER_HEIGHT) / 2)\n",
    "        self.basex = 0\n",
    "        self.baseShift = IMAGES['base'].get_width() - BACKGROUND_WIDTH\n",
    "\n",
    "        # generating random pipes for the gameplay. \n",
    "        newPipe1 = getRandomPipe()\n",
    "        newPipe2 = getRandomPipe()\n",
    "        self.upperPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[0]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[0]['y']},\n",
    "        ]\n",
    "        self.lowerPipes = [\n",
    "            {'x': SCREENWIDTH, 'y': newPipe1[1]['y']},\n",
    "            {'x': SCREENWIDTH + (SCREENWIDTH / 2), 'y': newPipe2[1]['y']},\n",
    "        ]\n",
    "\n",
    "        self.pipeVelX = -4\n",
    "        self.playerVelY    =  0    # player's velocity along Y, default same as playerFlapped\n",
    "        self.playerMaxVelY =  10   # max vel along Y, max descend speed\n",
    "        self.playerMinVelY =  -8   # min vel along Y, max ascend speed\n",
    "        self.playerAccY    =   1   # players downward accleration\n",
    "        self.playerFlapAcc =  -9   # players speed on flapping\n",
    "        self.playerFlapped = False # True when player flaps\n",
    "\n",
    "    def frame_step(self, input_actions):\n",
    "        pygame.event.pump()\n",
    "\n",
    "        reward = 0.1\n",
    "        terminal = False\n",
    "\n",
    "        if sum(input_actions) != 1:\n",
    "            raise ValueError('Multiple input actions!')\n",
    "\n",
    "        if input_actions[1] == 1:\n",
    "            if self.playery > -2 * PLAYER_HEIGHT:\n",
    "                self.playerVelY = self.playerFlapAcc\n",
    "                self.playerFlapped = True\n",
    "\n",
    "        # check for score\n",
    "        playerMidPos = self.playerx + PLAYER_WIDTH / 2\n",
    "        for pipe in self.upperPipes:\n",
    "            pipeMidPos = pipe['x'] + PIPE_WIDTH / 2\n",
    "            if pipeMidPos <= playerMidPos < pipeMidPos + 4:\n",
    "                self.score += 1\n",
    "                reward = 1\n",
    "\n",
    "        # playerIndex basex change\n",
    "        if (self.loopIter + 1) % 3 == 0:\n",
    "            self.playerIndex = next(PLAYER_INDEX_GEN)\n",
    "        self.loopIter = (self.loopIter + 1) % 30\n",
    "        self.basex = -((-self.basex + 100) % self.baseShift)\n",
    "\n",
    "        # player's movement\n",
    "        if self.playerVelY < self.playerMaxVelY and not self.playerFlapped:\n",
    "            self.playerVelY += self.playerAccY\n",
    "        if self.playerFlapped:\n",
    "            self.playerFlapped = False\n",
    "        self.playery += min(self.playerVelY, BASEY - self.playery - PLAYER_HEIGHT)\n",
    "        if self.playery < 0:\n",
    "            self.playery = 0\n",
    "\n",
    "        # move pipes to left\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            uPipe['x'] += self.pipeVelX\n",
    "            lPipe['x'] += self.pipeVelX\n",
    "\n",
    "        # add new pipe when first pipe is about to touch left of screen\n",
    "        if 0 < self.upperPipes[0]['x'] < 5:\n",
    "            newPipe = getRandomPipe()\n",
    "            self.upperPipes.append(newPipe[0])\n",
    "            self.lowerPipes.append(newPipe[1])\n",
    "\n",
    "        # remove first pipe if its out of the screen\n",
    "        if self.upperPipes[0]['x'] < -PIPE_WIDTH:\n",
    "            self.upperPipes.pop(0)\n",
    "            self.lowerPipes.pop(0)\n",
    "\n",
    "        # check if crash here\n",
    "        isCrash= checkCrash({'x': self.playerx, 'y': self.playery,\n",
    "                             'index': self.playerIndex},\n",
    "                            self.upperPipes, self.lowerPipes)\n",
    "        if isCrash:\n",
    "            terminal = True\n",
    "            self.__init__()\n",
    "            reward = -1\n",
    "\n",
    "        # draw sprites\n",
    "        SCREEN.blit(IMAGES['background'], (0,0))\n",
    "\n",
    "        for uPipe, lPipe in zip(self.upperPipes, self.lowerPipes):\n",
    "            SCREEN.blit(IMAGES['pipe'][0], (uPipe['x'], uPipe['y']))\n",
    "            SCREEN.blit(IMAGES['pipe'][1], (lPipe['x'], lPipe['y']))\n",
    "\n",
    "        SCREEN.blit(IMAGES['base'], (self.basex, BASEY))\n",
    "        # print score so player overlaps the score\n",
    "        showScore(self.score)\n",
    "        SCREEN.blit(IMAGES['player'][self.playerIndex],\n",
    "                    (self.playerx, self.playery))\n",
    "\n",
    "        image_data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
    "        pygame.display.update()\n",
    "        FPSCLOCK.tick(FPS)\n",
    "        return image_data, reward, terminal, self.score\n",
    "\n",
    "def getRandomPipe():\n",
    "    \"\"\"returns a randomly generated pipe\"\"\"\n",
    "    # y of gap between upper and lower pipe\n",
    "    gapYs = [20, 30, 40, 50, 60, 70, 80, 90]\n",
    "    index = random.randint(0, len(gapYs)-1)\n",
    "    gapY = gapYs[index]\n",
    "\n",
    "    gapY += int(BASEY * 0.2)\n",
    "    pipeX = SCREENWIDTH + 20\n",
    "\n",
    "    return [\n",
    "        {'x': pipeX, 'y': gapY - PIPE_HEIGHT},  # upper pipe\n",
    "        {'x': pipeX, 'y': gapY + PIPEGAPSIZE},  # lower pipe\n",
    "    ]\n",
    "\n",
    "\n",
    "def showScore(score):\n",
    "    \"\"\"displays score in center of screen\"\"\"\n",
    "    scoreDigits = [int(x) for x in list(str(score))]\n",
    "    totalWidth = 0 # total width of all numbers to be printed\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        totalWidth += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "    Xoffset = (SCREENWIDTH - totalWidth) / 2\n",
    "\n",
    "    for digit in scoreDigits:\n",
    "        SCREEN.blit(IMAGES['numbers'][digit], (Xoffset, SCREENHEIGHT * 0.1))\n",
    "        Xoffset += IMAGES['numbers'][digit].get_width()\n",
    "\n",
    "\n",
    "def checkCrash(player, upperPipes, lowerPipes):\n",
    "    \"\"\"returns True if player collders with base or pipes.\"\"\"\n",
    "    pi = player['index']\n",
    "    player['w'] = IMAGES['player'][0].get_width()\n",
    "    player['h'] = IMAGES['player'][0].get_height()\n",
    "\n",
    "    # if player crashes into ground\n",
    "    if player['y'] + player['h'] >= BASEY - 1:\n",
    "        return True\n",
    "    else:\n",
    "\n",
    "        playerRect = pygame.Rect(player['x'], player['y'],\n",
    "                      player['w'], player['h'])\n",
    "\n",
    "        for uPipe, lPipe in zip(upperPipes, lowerPipes):\n",
    "            # upper and lower pipe rects\n",
    "            uPipeRect = pygame.Rect(uPipe['x'], uPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "            lPipeRect = pygame.Rect(lPipe['x'], lPipe['y'], PIPE_WIDTH, PIPE_HEIGHT)\n",
    "\n",
    "            # player and upper/lower pipe hitmasks\n",
    "            pHitMask = HITMASKS['player'][pi]\n",
    "            uHitmask = HITMASKS['pipe'][0]\n",
    "            lHitmask = HITMASKS['pipe'][1]\n",
    "\n",
    "            # if bird collided with upipe or lpipe\n",
    "            uCollide = pixelCollision(playerRect, uPipeRect, pHitMask, uHitmask)\n",
    "            lCollide = pixelCollision(playerRect, lPipeRect, pHitMask, lHitmask)\n",
    "\n",
    "            if uCollide or lCollide:\n",
    "                return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def pixelCollision(rect1, rect2, hitmask1, hitmask2):\n",
    "    \"\"\"Checks if two objects collide and not just their rects\"\"\"\n",
    "    rect = rect1.clip(rect2)\n",
    "\n",
    "    if rect.width == 0 or rect.height == 0:\n",
    "        return False\n",
    "\n",
    "    x1, y1 = rect.x - rect1.x, rect.y - rect1.y\n",
    "    x2, y2 = rect.x - rect2.x, rect.y - rect2.y\n",
    "\n",
    "    for x in range(rect.width):\n",
    "        for y in range(rect.height):\n",
    "            if hitmask1[x1+x][y1+y] and hitmask2[x2+x][y2+y]:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, PyGame and the Flappy Bird Enviornment Is Ready. \n",
    "\n",
    "We can now define the hyperparameters that are involved in this experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# seed 1001 was the maximal performance seed. \n",
    "seed = 1001\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "\n",
    "# if you are running this on Google Colab (e.g., using Google Drive), enable to True. \n",
    "drive = False \n",
    "google_drive_colab_path = '/content/drive/My Drive/flappy/' if drive == True else ''\n",
    "\n",
    "OBSERVE = 10000 # timestpes to init the replay memory. \n",
    "EXPLORE = 1000000 # frames over which to decay epsilon\n",
    "\n",
    "FINAL_EPSILON = 0.0001 # final value\n",
    "INITIAL_EPSILON = 0.4 # starting value  \n",
    "\n",
    "REPLAY_MEMORY = 50000 # number of previous transitions to remember\n",
    "BATCH = 32 # size of minibatch\n",
    "FRAME_PER_ACTION = 1\n",
    "\n",
    "GAME = 'bird' # the name of the game being played for log files\n",
    "ACTIONS = 2 # number of valid actions\n",
    "GAMMA = 0.99 # decay rate of past observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below, you will find the `testing` variable. [[SKIP THIS SECTION IF YOU WOULD LIKE TO RUN THE TRAINING PROCESS, OTHERWISE, YOU MAY ENCOUNTER AN ERROR]]\n",
    "\n",
    "It's set to true initally. This is becuase, firstly, I will demonstrate the testing process to show reproducibility. Then, I will walk you through the training process.\n",
    "\n",
    "The network weights for the 130,000,000 frame of the seed 1001 are provided in the saved_networks folder. I didn't want to make the .zip or GitHub too large, so I just provided the best performing weights. Of course, more weights are available, please email me at: aadarsh.jha@vanderbilt.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testing = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 12:38:12.642664: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/util/tf_should_use.py:247: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "INFO:tensorflow:Restoring parameters from saved_networks/bird-dqn-1300000\n",
      "Successfully loaded: saved_networks/bird-dqn-1300000\n",
      "TIMESTEP, 1529 Reward, 186.89999999999495 Average Reward, 186.89999999999495 Flaps, 39 Average Flaps, 39.0 Standard Deviation Flaps, 0.0\n",
      "TIMESTEP, 3667 Reward, 262.19999999999334 Average Reward, 224.54999999999416 Flaps, 55 Average Flaps, 47.0 Standard Deviation Flaps, 8.0\n",
      "TIMESTEP, 8465 Reward, 591.2000000000522 Average Reward, 346.76666666668024 Flaps, 125 Average Flaps, 73.0 Standard Deviation Flaps, 37.345236197762446\n",
      "TIMESTEP, 12732 Reward, 525.5000000000405 Average Reward, 391.4500000000203 Flaps, 111 Average Flaps, 82.5 Standard Deviation Flaps, 36.2870500316573\n",
      "TIMESTEP, 14338 Reward, 196.39999999999452 Average Reward, 352.4400000000152 Flaps, 41 Average Flaps, 74.2 Standard Deviation Flaps, 36.454903648206226\n",
      "TIMESTEP, 16249 Reward, 234.09999999999283 Average Reward, 332.7166666666781 Flaps, 49 Average Flaps, 70.0 Standard Deviation Flaps, 34.578413304642346\n",
      "TIMESTEP, 18387 Reward, 262.19999999999334 Average Reward, 322.642857142866 Flaps, 55 Average Flaps, 67.85714285714286 Standard Deviation Flaps, 32.440840976542304\n",
      "TIMESTEP, 22997 Reward, 567.9000000000481 Average Reward, 353.3000000000137 Flaps, 120 Average Flaps, 74.375 Standard Deviation Flaps, 34.90321439351969\n",
      "TIMESTEP, 26391 Reward, 417.50000000002115 Average Reward, 360.43333333334783 Flaps, 88 Average Flaps, 75.88888888888889 Standard Deviation Flaps, 33.1844824661047\n",
      "TIMESTEP, 26823 Reward, 51.10000000000038 Average Reward, 329.5000000000131 Flaps, 10 Average Flaps, 69.3 Standard Deviation Flaps, 37.17270504012319\n",
      "TIMESTEP, 30617 Reward, 467.40000000003 Average Reward, 342.03636363637827 Flaps, 99 Average Flaps, 72.0 Standard Deviation Flaps, 36.45669909757203\n",
      "Testing Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def createNetwork():\n",
    "    # network weights\n",
    "    weight_conv_1 = weight_variable([8, 8, 4, 32])\n",
    "    bias_conv_1 = bias_variable([32])\n",
    "\n",
    "    weight_conv_2 = weight_variable([4, 4, 32, 64])\n",
    "    bias_conv_2 = bias_variable([64])\n",
    "\n",
    "    weight_conv_3 = weight_variable([3, 3, 64, 64])\n",
    "    bias_conv_3 = bias_variable([64])\n",
    "\n",
    "    weight_fully_1 = weight_variable([1600, 512])\n",
    "    bias_fully_1 = bias_variable([512])\n",
    "\n",
    "    weight_fully_2 = weight_variable([512, ACTIONS])\n",
    "    bias_fully_2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # input layer\n",
    "    input_layer = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # hidden layers\n",
    "    hidden_conv_1 = tf.nn.relu(conv2d(input_layer, weight_conv_1, 4) + bias_conv_1)\n",
    "    pool_1 = max_pool_2x2(hidden_conv_1)\n",
    "\n",
    "    hidden_conv_2 = tf.nn.relu(conv2d(pool_1, weight_conv_2, 2) + bias_conv_2)\n",
    "    hidden_conv_3 = tf.nn.relu(conv2d(hidden_conv_2, weight_conv_3, 1) + bias_conv_3)\n",
    "\n",
    "    hidden_conv_3_flat = tf.reshape(hidden_conv_3, [-1, 1600])\n",
    "\n",
    "    hidden_fully_connected_1 = tf.nn.relu(tf.matmul(hidden_conv_3_flat, weight_fully_1) + bias_fully_1)\n",
    "\n",
    "    readout = tf.matmul(hidden_fully_connected_1, weight_fully_2) + bias_fully_2\n",
    "\n",
    "    return input_layer, readout, hidden_fully_connected_1\n",
    "\n",
    "def trainNetwork(s, readout, _, sess):\n",
    "    counter = 0 \n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "    # readout stores the output of the network. \n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "\n",
    "    # the optimizer is declared. \n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # open up a game state \n",
    "    game_state = GameState()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "\n",
    "    # preprocess the image to 80x80x4 and get the image state. \n",
    "    x_t, _, terminal, _ = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "\n",
    "    # are we testing or training? the decision is made here. \n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    # start training\n",
    "\n",
    "    # data structures meant for logging\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    score = [] \n",
    "    net_score = [] \n",
    "    net_flaps = [] \n",
    "    flaps = [] \n",
    "\n",
    "    # we continue to execute forever, until the game ends.\n",
    "    while True:\n",
    "\n",
    "        # get all the actions from the netwokr \n",
    "        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "\n",
    "        # if we're testing we dont need to follow an epsilon greedy policy.\n",
    "        # just get the highest action value. \n",
    "        if testing:\n",
    "            if counter > 10: \n",
    "                print(\"Testing Done\")\n",
    "                return\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                a_t[0] = 1 \n",
    "        else: \n",
    "            # otherwise, we should select randomly at times. (Defined by epsilon)\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                if random.random() <= epsilon:\n",
    "                    print(\"Random Action Selected Via Epsilon Greedy\")\n",
    "                    action_index = random.randrange(ACTIONS)\n",
    "                    a_t[random.randrange(ACTIONS)] = 1\n",
    "                else:\n",
    "                    action_index = np.argmax(readout_t)\n",
    "                    a_t[action_index] = 1\n",
    "            else:\n",
    "                a_t[0] = 1 # do nothing\n",
    "\n",
    "        # downscale the value of the epsilon. \n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_colored, r_t, terminal, cur_score = game_state.frame_step(a_t)\n",
    "        flaps.append(cur_score)\n",
    "\n",
    "        # process the image to 80x80x4 to preparer to feed into the network. \n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        _, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "        \n",
    "        score.append(r_t)\n",
    "\n",
    "        # we store memory via the replay memory.\n",
    "        if testing == False: \n",
    "\n",
    "            # store the transition in D\n",
    "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "\n",
    "            # popping when above the memory. \n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "            # only train if done observing (We've sufficiently filled the replay memory)\n",
    "            if t > OBSERVE:\n",
    "                # sample a minibatch to train on\n",
    "                minibatch = random.sample(D, BATCH)\n",
    "\n",
    "                # get the batch variables\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                r_batch = [d[2] for d in minibatch]\n",
    "                s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "                y_batch = []\n",
    "\n",
    "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "                \n",
    "                for i in range(0, len(minibatch)):\n",
    "                    terminal = minibatch[i][4]\n",
    "                    # if terminal, only equals reward\n",
    "                    if terminal:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "                # perform gradient step\n",
    "                train_step.run(feed_dict = {\n",
    "                    y : y_batch,\n",
    "                    a : a_batch,\n",
    "                    s : s_j_batch}\n",
    "                )\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        if testing == False: \n",
    "            # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                saver.save(sess, google_drive_colab_path + 'saved_networks_v1/' + GAME + '-dqn', global_step = t)\n",
    "\n",
    "            # print info\n",
    "            state = \"\"\n",
    "            if t <= OBSERVE:\n",
    "                state = \"observe\"\n",
    "            elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "                \n",
    "        if terminal: \n",
    "            net_score.append(sum(score))\n",
    "            net_flaps.append(max(flaps))\n",
    "\n",
    "        if terminal and testing: \n",
    "            counter = counter + 1\n",
    "            print(\"TIMESTEP,\", t, \"Reward,\", sum(score), \"Average Reward,\", np.mean(net_score), \"Flaps,\", max(flaps), \"Average Flaps,\", np.mean(net_flaps), \"Standard Deviation Flaps,\", np.std(net_flaps))\n",
    "            score = []    \n",
    "            flaps = [] \n",
    "\n",
    "        if terminal and testing == False: \n",
    "            string = \"GameOver TIMESTEP: \" + str(t) + \", STATE: \" + str(state) + \", EPSILON: \" + str(epsilon) + \", ACTION: \" + str(action_index) + \", REWARD: \" + str(r_t) + \", Q_MAX: %e\" % np.max(readout_t) + \", Episode Reward: \" + str(sum(score)) +  \", Average Reward: \" + str(np.mean(net_score)) + \", Standard Deviation Of Score: \" + str(np.std(net_score)) + \", Flaps: \" + str(max(flaps)) +  \", Average Flaps: \" + str(np.mean(net_flaps)) + \", Standard Deviation Of Flaps: \" + str(np.std(net_flaps))\n",
    "            print(string)\n",
    "            print(\"Game Over\")\n",
    "            with open(google_drive_colab_path + \"net_score_cache_v1_game_over.txt\", 'a') as f:\n",
    "                f.write(string + \"\\n\")\n",
    "                f.close()\n",
    "            score = [] \n",
    "            flaps = [] \n",
    "\n",
    "        if terminal == False and testing == False: \n",
    "            string = \"TIMESTEP: \" + str(t) + \", STATE: \" + str(state) + \", EPSILON: \" + str(epsilon) + \", ACTION: \" + str(action_index) + \", REWARD: \" + str(r_t) + \", Q_MAX: %e\" % np.max(readout_t) + \", Episode Reward: \" + str(sum(score)) +  \", Average Reward: \" + str(np.mean(net_score)) + \", Standard Deviation Of Score: \" + str(np.std(net_score)) + \", Flaps: \" + str(max(flaps)) +  \", Average Flaps: \" + str(np.mean(net_flaps)) + \", Standard Deviation Of Flaps: \" + str(np.std(net_flaps))\n",
    "            print(string) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sess = tf.InteractiveSession()\n",
    "    input_layer, readout, hidden_fully_connected_1 = createNetwork()\n",
    "    trainNetwork(input_layer, readout, hidden_fully_connected_1, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Above, the testing is executed. \n",
    "\n",
    "To interpret this a bit better, here's a summary of the process:\n",
    "* A log is produced everytime the game is reset (e.g., the bird dies). \n",
    "* Reward, and average reward are computed for reference. \n",
    "* What is most important to note is the Flaps, Average Flaps, and Standard Deviation of Flaps, as they represent the number of times the bird flaps, which is the primary, and only, metric of how well the user is at playing the game. \n",
    "\n",
    "Referencing the paper, one can see how the maximal flaps are: 125, and the average flaps are: 72.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we can move onto training. \n",
    "\n",
    "* We switch the `testing` variable to `True` to enable the training process.\n",
    "* It is recommended to use a GPU for training. CPU will take roughly 2 days, whereas GPU will take 1 day. \n",
    "* The block from above is copied again, since its the same code yet again to train, except we set the `testing` variable to `False`.\n",
    "* If an error is encountered wherein tensorflow says a session is already in place, simply restart the notebook and do NOT run the testing code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = \"SAME\")\n",
    "\n",
    "def createNetwork():\n",
    "    # network weights\n",
    "    weight_conv_1 = weight_variable([8, 8, 4, 32])\n",
    "    bias_conv_1 = bias_variable([32])\n",
    "\n",
    "    weight_conv_2 = weight_variable([4, 4, 32, 64])\n",
    "    bias_conv_2 = bias_variable([64])\n",
    "\n",
    "    weight_conv_3 = weight_variable([3, 3, 64, 64])\n",
    "    bias_conv_3 = bias_variable([64])\n",
    "\n",
    "    weight_fully_1 = weight_variable([1600, 512])\n",
    "    bias_fully_1 = bias_variable([512])\n",
    "\n",
    "    weight_fully_2 = weight_variable([512, ACTIONS])\n",
    "    bias_fully_2 = bias_variable([ACTIONS])\n",
    "\n",
    "    # input layer\n",
    "    input_layer = tf.placeholder(\"float\", [None, 80, 80, 4])\n",
    "\n",
    "    # hidden layers\n",
    "    hidden_conv_1 = tf.nn.relu(conv2d(input_layer, weight_conv_1, 4) + bias_conv_1)\n",
    "    pool_1 = max_pool_2x2(hidden_conv_1)\n",
    "\n",
    "    hidden_conv_2 = tf.nn.relu(conv2d(pool_1, weight_conv_2, 2) + bias_conv_2)\n",
    "    hidden_conv_3 = tf.nn.relu(conv2d(hidden_conv_2, weight_conv_3, 1) + bias_conv_3)\n",
    "\n",
    "    hidden_conv_3_flat = tf.reshape(hidden_conv_3, [-1, 1600])\n",
    "\n",
    "    hidden_fully_connected_1 = tf.nn.relu(tf.matmul(hidden_conv_3_flat, weight_fully_1) + bias_fully_1)\n",
    "\n",
    "    readout = tf.matmul(hidden_fully_connected_1, weight_fully_2) + bias_fully_2\n",
    "\n",
    "    return input_layer, readout, hidden_fully_connected_1\n",
    "\n",
    "def trainNetwork(s, readout, _, sess):\n",
    "    counter = 0 \n",
    "    # define the cost function\n",
    "    a = tf.placeholder(\"float\", [None, ACTIONS])\n",
    "    y = tf.placeholder(\"float\", [None])\n",
    "\n",
    "    # readout stores the output of the network. \n",
    "    readout_action = tf.reduce_sum(tf.multiply(readout, a), reduction_indices=1)\n",
    "    cost = tf.reduce_mean(tf.square(y - readout_action))\n",
    "\n",
    "    # the optimizer is declared. \n",
    "    train_step = tf.train.AdamOptimizer(1e-6).minimize(cost)\n",
    "\n",
    "    # open up a game state \n",
    "    game_state = GameState()\n",
    "\n",
    "    # store the previous observations in replay memory\n",
    "    D = deque()\n",
    "\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "    do_nothing = np.zeros(ACTIONS)\n",
    "    do_nothing[0] = 1\n",
    "\n",
    "    # preprocess the image to 80x80x4 and get the image state. \n",
    "    x_t, _, terminal, _ = game_state.frame_step(do_nothing)\n",
    "    x_t = cv2.cvtColor(cv2.resize(x_t, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "    ret, x_t = cv2.threshold(x_t,1,255,cv2.THRESH_BINARY)\n",
    "    s_t = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "\n",
    "    # saving and loading networks\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    checkpoint = tf.train.get_checkpoint_state(\"saved_networks\")\n",
    "\n",
    "    # are we testing or training? the decision is made here. \n",
    "    if checkpoint and checkpoint.model_checkpoint_path:\n",
    "        saver.restore(sess, checkpoint.model_checkpoint_path)\n",
    "        print(\"Successfully loaded:\", checkpoint.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Could not find old network weights\")\n",
    "\n",
    "    # start training\n",
    "\n",
    "    # data structures meant for logging\n",
    "    epsilon = INITIAL_EPSILON\n",
    "    t = 0\n",
    "    score = [] \n",
    "    net_score = [] \n",
    "    net_flaps = [] \n",
    "    flaps = [] \n",
    "\n",
    "    # we continue to execute forever, until the game ends.\n",
    "    while True:\n",
    "\n",
    "        # get all the actions from the netwokr \n",
    "        readout_t = readout.eval(feed_dict={s : [s_t]})[0]\n",
    "\n",
    "        a_t = np.zeros([ACTIONS])\n",
    "        action_index = 0\n",
    "\n",
    "        # if we're testing we dont need to follow an epsilon greedy policy.\n",
    "        # just get the highest action value. \n",
    "        if testing:\n",
    "            if counter > 10: \n",
    "                print(\"Testing Done\")\n",
    "                return\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                action_index = np.argmax(readout_t)\n",
    "                a_t[action_index] = 1\n",
    "            else:\n",
    "                a_t[0] = 1 \n",
    "        else: \n",
    "            # otherwise, we should select randomly at times. (Defined by epsilon)\n",
    "            if t % FRAME_PER_ACTION == 0:\n",
    "                if random.random() <= epsilon:\n",
    "                    print(\"Random Action Selected Via Epsilon Greedy\")\n",
    "                    action_index = random.randrange(ACTIONS)\n",
    "                    a_t[random.randrange(ACTIONS)] = 1\n",
    "                else:\n",
    "                    action_index = np.argmax(readout_t)\n",
    "                    a_t[action_index] = 1\n",
    "            else:\n",
    "                a_t[0] = 1 # do nothing\n",
    "\n",
    "        # downscale the value of the epsilon. \n",
    "        if epsilon > FINAL_EPSILON and t > OBSERVE:\n",
    "            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE\n",
    "\n",
    "        # run the selected action and observe next state and reward\n",
    "        x_t1_colored, r_t, terminal, cur_score = game_state.frame_step(a_t)\n",
    "        flaps.append(cur_score)\n",
    "\n",
    "        # process the image to 80x80x4 to preparer to feed into the network. \n",
    "        x_t1 = cv2.cvtColor(cv2.resize(x_t1_colored, (80, 80)), cv2.COLOR_BGR2GRAY)\n",
    "        _, x_t1 = cv2.threshold(x_t1, 1, 255, cv2.THRESH_BINARY)\n",
    "        x_t1 = np.reshape(x_t1, (80, 80, 1))\n",
    "        s_t1 = np.append(x_t1, s_t[:, :, :3], axis=2)\n",
    "        \n",
    "        score.append(r_t)\n",
    "\n",
    "        # we store memory via the replay memory.\n",
    "        if testing == False: \n",
    "\n",
    "            # store the transition in D\n",
    "            D.append((s_t, a_t, r_t, s_t1, terminal))\n",
    "\n",
    "            # popping when above the memory. \n",
    "            if len(D) > REPLAY_MEMORY:\n",
    "                D.popleft()\n",
    "\n",
    "            # only train if done observing (We've sufficiently filled the replay memory)\n",
    "            if t > OBSERVE:\n",
    "                # sample a minibatch to train on\n",
    "                minibatch = random.sample(D, BATCH)\n",
    "\n",
    "                # get the batch variables\n",
    "                s_j_batch = [d[0] for d in minibatch]\n",
    "                a_batch = [d[1] for d in minibatch]\n",
    "                r_batch = [d[2] for d in minibatch]\n",
    "                s_j1_batch = [d[3] for d in minibatch]\n",
    "\n",
    "                y_batch = []\n",
    "\n",
    "                readout_j1_batch = readout.eval(feed_dict = {s : s_j1_batch})\n",
    "                \n",
    "                for i in range(0, len(minibatch)):\n",
    "                    terminal = minibatch[i][4]\n",
    "                    # if terminal, only equals reward\n",
    "                    if terminal:\n",
    "                        y_batch.append(r_batch[i])\n",
    "                    else:\n",
    "                        y_batch.append(r_batch[i] + GAMMA * np.max(readout_j1_batch[i]))\n",
    "\n",
    "                # perform gradient step\n",
    "                train_step.run(feed_dict = {\n",
    "                    y : y_batch,\n",
    "                    a : a_batch,\n",
    "                    s : s_j_batch}\n",
    "                )\n",
    "\n",
    "        # update the old values\n",
    "        s_t = s_t1\n",
    "        t += 1\n",
    "\n",
    "        if testing == False: \n",
    "            # save progress every 10000 iterations\n",
    "            if t % 10000 == 0:\n",
    "                saver.save(sess, google_drive_colab_path + 'saved_networks_v1/' + GAME + '-dqn', global_step = t)\n",
    "\n",
    "            # print info\n",
    "            state = \"\"\n",
    "            if t <= OBSERVE:\n",
    "                state = \"observe\"\n",
    "            elif t > OBSERVE and t <= OBSERVE + EXPLORE:\n",
    "                state = \"explore\"\n",
    "            else:\n",
    "                state = \"train\"\n",
    "                \n",
    "        if terminal: \n",
    "            net_score.append(sum(score))\n",
    "            net_flaps.append(max(flaps))\n",
    "\n",
    "        if terminal and testing: \n",
    "            counter = counter + 1\n",
    "            print(\"TIMESTEP,\", t, \"Reward,\", sum(score), \"Average Reward,\", np.mean(net_score), \"Flaps,\", max(flaps), \"Average Flaps,\", np.mean(net_flaps), \"Standard Deviation Flaps,\", np.std(net_flaps))\n",
    "            score = []    \n",
    "            flaps = [] \n",
    "\n",
    "        if terminal and testing == False: \n",
    "            string = \"GameOver TIMESTEP: \" + str(t) + \", STATE: \" + str(state) + \", EPSILON: \" + str(epsilon) + \", ACTION: \" + str(action_index) + \", REWARD: \" + str(r_t) + \", Q_MAX: %e\" % np.max(readout_t) + \", Episode Reward: \" + str(sum(score)) +  \", Average Reward: \" + str(np.mean(net_score)) + \", Standard Deviation Of Score: \" + str(np.std(net_score)) + \", Flaps: \" + str(max(flaps)) +  \", Average Flaps: \" + str(np.mean(net_flaps)) + \", Standard Deviation Of Flaps: \" + str(np.std(net_flaps))\n",
    "            print(string)\n",
    "            print(\"Game Over\")\n",
    "            with open(google_drive_colab_path + \"net_score_cache_v1_game_over.txt\", 'a') as f:\n",
    "                f.write(string + \"\\n\")\n",
    "                f.close()\n",
    "            score = [] \n",
    "            flaps = [] \n",
    "\n",
    "        if terminal == False and testing == False: \n",
    "            string = \"TIMESTEP: \" + str(t) + \", STATE: \" + str(state) + \", EPSILON: \" + str(epsilon) + \", ACTION: \" + str(action_index) + \", REWARD: \" + str(r_t) + \", Q_MAX: %e\" % np.max(readout_t) + \", Episode Reward: \" + str(sum(score)) +  \", Average Reward: \" + str(np.mean(net_score)) + \", Standard Deviation Of Score: \" + str(np.std(net_score)) + \", Flaps: \" + str(max(flaps)) +  \", Average Flaps: \" + str(np.mean(net_flaps)) + \", Standard Deviation Of Flaps: \" + str(np.std(net_flaps))\n",
    "            print(string) \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sess = tf.InteractiveSession()\n",
    "    input_layer, readout, hidden_fully_connected_1 = createNetwork()\n",
    "    trainNetwork(input_layer, readout, hidden_fully_connected_1, sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# While the training is running...\n",
    "You will notice the following files being created: \n",
    "* `net_score_cache_v1_game_over.txt` -- This contains a log of every each timestep (frame) that is processed. \n",
    "* `saved_networks_v1/` -- This contains the checkpointed weights of the network.\n",
    "\n",
    "The `net_score_cache_v1_game_over.txt` was used for logging. `saved_networks_v1/` was used for testing, as seen above in the notebook. \n",
    "**Note `saved_networks_v1/` will note be created until 10,000 frames have been processed.**\n",
    "\n",
    "Additionally, it is reecommended that you early stop the network at 10,000 (so the above error is normal and fine) for the sake of learning in this enviornment. In reality, this was not stopped so early (rather, 1,300,000 timesteps were ran). This is so you can have some models for yourself to graph and play around with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, we can plot. \n",
    "* The results of the log are used to plot. Of course, the graphs do not look complete, but are meant as a playground of code rather than a representation of the actual results that was fed from the training process. Logs, graphs, and plots are all avaliable in the `deep-q-learning-results` folder in the root of the repo. \n",
    "\n",
    "* Note! You must convert the .txt file to a .csv file before running the below code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file with pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(d1, d2, xlab, ylab, title, file): \n",
    "    plt.plot(d1, d2)\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    plt.title(title)\n",
    "    plt.savefig(file)\n",
    "    plt.close()\n",
    "\n",
    "# read net_score_cache_game_over\n",
    "df = pd.read_csv('net_score_cache_v1_game_over.csv')\n",
    "\n",
    "# the first column is the timestamp\n",
    "timestamp = df.iloc[:, 0]\n",
    "\n",
    "#extract number out of timestamp\n",
    "timestamp_number = []\n",
    "\n",
    "for element in timestamp: \n",
    "    # iterate until the element is a number\n",
    "    number = \"\"\n",
    "    for char in element:\n",
    "        if char.isdigit(): \n",
    "            number += char\n",
    "    timestamp_number.append(int(number))\n",
    "\n",
    "#extract average reward\n",
    "reward = df.iloc[:, 6]\n",
    "reward_number = []\n",
    "\n",
    "for element in reward:\n",
    "    number = \"\" \n",
    "    for i in range(len(element)): \n",
    "        if element[i].isdigit() or element[i] == '-': \n",
    "            number = element[i:]\n",
    "            reward_number.append(float(number))\n",
    "            break\n",
    "\n",
    "#extract average reward\n",
    "average_reward = df.iloc[:, 7]\n",
    "average_reward_number = []\n",
    "\n",
    "for element in average_reward:\n",
    "    number = \"\" \n",
    "    for i in range(len(element)): \n",
    "        if element[i].isdigit() or element[i] == \"-\": \n",
    "            number = element[i:]\n",
    "            average_reward_number.append(float(number))\n",
    "            if (float(number) < 0): \n",
    "                print(number)\n",
    "            break\n",
    "\n",
    "#extract standard deviation of reward\n",
    "std_reward = df.iloc[:, 8]\n",
    "std_reward_number = []\n",
    "\n",
    "for element in std_reward:\n",
    "    number = \"\" \n",
    "\n",
    "    for i in range(len(element)): \n",
    "        if element[i].isdigit(): \n",
    "            number = element[i:]\n",
    "            std_reward_number.append(float(number))\n",
    "            break\n",
    "\n",
    "#plot reward vs timestamp\n",
    "plot(timestamp_number, reward_number, \"Timestamp\", \"Reward\", \"Reward vs Timestamp\", \"reward_vs_timestamp.png\")\n",
    "\n",
    "#plot average_reward_number vs timestamp\n",
    "plot(timestamp_number, average_reward_number, \"Timestamp\", \"Average Reward\", \"Average Reward vs Timestamp\", \"average_reward_vs_timestamp.png\")\n",
    "\n",
    "#plot std_reward_number vs timestamp\n",
    "plot(timestamp_number, std_reward_number, \"Timestamp\", \"Standard Deviation of Reward\", \"Standard Deviation of Reward vs Timestamp\", \"std_reward_vs_timestamp.png\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f90423e2e0c44cf48db938ae8f82643fef78d8d86df0dbda041ec903cb829d6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
